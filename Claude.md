# Servyy Container - Project Documentation

> **Last Updated:** 2025-11-03
> **Version:** Main documentation for servyy-container infrastructure

---

## Table of Contents

1. [Overview](#overview)
2. [Architecture](#architecture)
3. [Service Naming Convention](#service-naming-convention)
4. [Services](#services)
5. [Ansible Automation](#ansible-automation)
6. [Scripts & Utilities](#scripts--utilities)
7. [Configuration Management](#configuration-management)
8. [Networking](#networking)
9. [Deployment](#deployment)
10. [Monitoring & Backup](#monitoring--backup)
11. [Development Workflow](#development-workflow)

---

## Overview

**servyy-container** is a production-grade, self-hosted microservices platform built on Docker and automated with Ansible. It manages 15+ containerized services including photo library management, social networking, DNS filtering, monitoring, and logging.

### Key Features

- **Infrastructure as Code:** Complete automation from OS to services
- **Security First:** git-crypt encryption, SSH hardening, automated firewalls
- **Self-Hosted:** No dependency on external SaaS platforms
- **Multi-Environment:** Production servers + LXD test containers
- **Comprehensive Monitoring:** monit, Prometheus, Grafana, centralized logging
- **Automated Backups:** Daily backups to Hetzner Storagebox

### Technology Stack

| Layer | Technology |
|-------|-----------|
| Container Runtime | Docker, Docker Compose |
| Orchestration | Docker Compose (per-service) |
| Reverse Proxy | Traefik |
| Automation | Ansible |
| Secrets | git-crypt |
| Monitoring | Prometheus, Grafana, monit |
| Backup | rsync to Hetzner Storagebox |
| Testing | LXD containers |

---

## Architecture

### High-Level Design

```
┌─────────────────────────────────────────────────────────────┐
│                         Internet                            │
└────────────────┬────────────────────────────────────────────┘
                 │
        ┌────────▼─────────┐
        │   Porkbun DNS    │ (lehel.xyz domain)
        │  Hetzner Firewall│
        └────────┬─────────┘
                 │
        ┌────────▼─────────┐
        │     Traefik      │ (Reverse Proxy & SSL)
        │  lehel.xyz:443   │
        └────────┬─────────┘
                 │
    ┌────────────▼───────────────┐
    │ Docker Network: "proxy"    │
    └────────────┬───────────────┘
                 │
    ┌────────────┴───────────────────────────────────┐
    │                                                 │
┌───▼───┐  ┌─────▼──────┐  ┌──────▼──────┐  ┌──────▼──────┐
│ Photo │  │   Social   │  │     Git     │  │   Monitor   │
│ prism │  │  (Pleroma) │  │   (Gitea)   │  │ (Prometheus)│
└───┬───┘  └─────┬──────┘  └──────┬──────┘  └──────┬──────┘
    │            │                 │                 │
    │        ┌───▼──┐          ┌───▼──┐          ┌──▼──┐
    │        │ DB   │          │nginx │          │cAdv │
    │        └──────┘          └──────┘          └─────┘
    │
    ▼
┌───────────────────────────┐
│ /mnt/storagebox (CIFS)    │
│ - Daily backups           │
│ - Photo storage           │
│ - Database backups        │
└───────────────────────────┘
```

### Directory Structure

```
servyy-container/
├── achim-hoefer/        # Personal website (Parcel bundled)
├── ansible/             # Infrastructure automation
│   ├── plays/          # Playbooks (system, user, testing)
│   ├── inventory/      # Host definitions
│   └── requirements.yml
├── bumbleflies/         # Custom application (nginx)
├── dns/                 # PiHole DNS + ad blocking
├── duckdns/            # Legacy: Dynamic DNS (deprecated, use Porkbun DNS)
├── energy/             # Energy monitoring
├── git/                # Git hosting (nginx)
├── hetzner/            # Hetzner Cloud firewall automation
├── jobs/               # Job scheduler + Google API
├── logging/            # Centralized logging (ELK)
├── me/                 # Personal website (nginx)
├── monitor/            # Prometheus + Grafana + exporters
├── pass/               # Password management
├── photoprism/         # AI-powered photo library
├── portainer/          # Docker UI management
├── scripts/            # Utility scripts
│   ├── etc/           # cloud-config.yaml
│   ├── setup_test_container.sh
│   ├── startup_docker_services.sh
│   └── cleanup_space.sh
├── social/             # Pleroma federated social network
└── traefik/            # Reverse proxy configuration

Each service directory contains:
- docker-compose.yml (encrypted with git-crypt)
- .env files (generated by Ansible)
- Service-specific configs (nginx.conf, etc.)
```

---

## Service Naming Convention

### Overview

The servyy-container project uses a **systematic naming convention** that automatically generates service hostnames based on the directory structure and inventory hostname. This convention ensures consistent, predictable URLs for all services.

### Naming Pattern

```
Service URL: {directory-name}.{inventory-hostname}
```

**Formula:**
- **SERVICE_NAME** = Directory name (e.g., `photoprism`, `git`, `social`)
- **SERVICE_HOST** = `{directory-name}.{inventory-hostname}`
- **Inventory Hostname** = `lehel.xyz` (production server)

**Example:**
```
Directory: photoprism/
Inventory Host: lehel.xyz
→ SERVICE_NAME: photoprism
→ SERVICE_HOST: photoprism.lehel.xyz
→ Full URL: https://photoprism.lehel.xyz
```

### How It Works

**1. Ansible Discovers Services:**
   - Ansible scans all directories in the repository
   - Each directory with a `docker-compose.yml` = one service
   - Directory name becomes the service name

**2. Environment File Generation:**
   - Template: `ansible/plays/roles/user/templates/docker.env.j2`
   - Generated for each service: `{service-dir}/.env`

   ```bash
   SERVICE_HOST={{item.dir}}.{{inventory_hostname}}
   SERVICE_NAME={{item.dir}}
   LOCAL_HOSTNAME={{inventory_hostname_short}}
   IPV4_HOST={{ lookup('dig', ansible_host) }}
   UID={{ user.id }}
   GID={{ user.group }}
   ```

**3. Traefik Routes Traffic:**
   - Docker Compose labels define routing rules
   - Traefik automatically discovers services
   - Routes based on `Host()` matcher

   ```yaml
   labels:
     - "traefik.enable=true"
     - "traefik.http.routers.{service}.rule=Host(`{service}.lehel.xyz`)"
     - "traefik.http.routers.{service}.entrypoints=websecure"
     - "traefik.http.routers.{service}.tls.certresolver=letsencrypt"
   ```

### Complete Service Directory & URL Mapping

**Production Server: `lehel.xyz`**

| Directory | Service Name | Service URL | Purpose |
|-----------|--------------|-------------|---------|
| `achim-hoefer/` | achim-hoefer | https://achim-hoefer.lehel.xyz | Personal website (Parcel) |
| `bumbleflies/` | bumbleflies | https://bumbleflies.lehel.xyz | Custom application |
| `dns/` | dns | https://dns.lehel.xyz | PiHole admin interface |
| `energy/` | energy | https://energy.lehel.xyz | Energy monitoring |
| `git/` | git | https://git.lehel.xyz | Git repository hosting |
| `jobs/` | jobs | https://jobs.lehel.xyz | Job scheduler |
| `logging/` | logging | https://logging.lehel.xyz | Centralized logging UI |
| `me/` | me | https://me.lehel.xyz | Personal website |
| `monitor/` | monitor | https://monitor.lehel.xyz | Grafana dashboard |
| `pass/` | pass | https://pass.lehel.xyz | Password manager |
| `photoprism/` | photoprism | https://photoprism.lehel.xyz | Photo library |
| `portainer/` | portainer | https://portainer.lehel.xyz | Docker UI |
| `social/` | social | https://social.lehel.xyz | Pleroma social network |
| `traefik/` | traefik | https://traefik.lehel.xyz | Traefik dashboard |

**Special Services (Non-Web):**

| Directory | Purpose | Access Method |
|-----------|---------|---------------|
| `duckdns/` | ~~Dynamic DNS updater~~ (deprecated) | Legacy - domain now via Porkbun |
| `hetzner/` | Firewall automation | CLI scripts only |
| `scripts/` | Utility scripts | Shell scripts |
| `ansible/` | Infrastructure automation | Playbooks |

### Domain Resolution Flow

```
User Browser
    ↓
https://photoprism.lehel.xyz
    ↓
[DNS Resolution]
    ├─ External: Porkbun DNS → lehel.xyz → Server IP
    └─ Internal: PiHole → 192.168.x.x (local network)
    ↓
[Server: lehel.xyz]
    ↓
[Traefik Ingress]
    ├─ Matches: Host(`photoprism.lehel.xyz`)
    ├─ Routes to: photoprism container :8080
    └─ SSL/TLS: Let's Encrypt certificate
    ↓
[PhotoPrism Container]
```

### DNS Management via Porkbun

**Primary Domain:** `lehel.xyz` (owned domain via Porkbun registrar)
**DNS Provider:** Porkbun DNS

**DNS Configuration:**
- **Domain Registrar:** Porkbun
- **DNS Records:** Managed through Porkbun DNS console
- **A Record:** `lehel.xyz` → Server IPv4
- **AAAA Record:** `lehel.xyz` (optional) → Server IPv6
- **Wildcard:** `*.lehel.xyz` → Server IP (for all service subdomains)

**Service Access:**
All services are accessible via their subdomain:
```
https://photoprism.lehel.xyz
https://git.lehel.xyz
https://social.lehel.xyz
https://monitor.lehel.xyz
... etc
```

**Access Methods:**
- **External:** `https://{service}.lehel.xyz` → Porkbun DNS → Server → Traefik → Service
- **Local Network:** Internal IP with service subdomains via PiHole

**Legacy Note:**
- The `duckdns/` directory contains legacy dynamic DNS scripts
- No longer used since purchasing `lehel.xyz` domain through Porkbun
- DNS updates now managed through Porkbun's DNS console

### Multiple Environments

The naming convention adapts to different inventory hosts:

**Production (`lehel.xyz`):**
```
photoprism.lehel.xyz
git.lehel.xyz
social.lehel.xyz
```

**Development (`aqui.fritz.box`):**
```
photoprism.aqui.fritz.box
git.aqui.fritz.box
social.aqui.fritz.box
```

**Test Container (`servyy-test.lxd`):**
```
photoprism.servyy-test.lxd
git.servyy-test.lxd
social.servyy-test.lxd
```

### Customizing Service Names

To add a new service with custom naming:

**Option 1: Use Directory Name (Automatic)**
```bash
# Create directory
mkdir my-service/

# Create docker-compose.yml with traefik labels
# Result: my-service.lehel.xyz
```

**Option 2: Override in Docker Compose**
```yaml
# my-service/docker-compose.yml
services:
  app:
    labels:
      - "traefik.http.routers.custom.rule=Host(`custom-name.lehel.xyz`)"
```

**Option 3: Use Environment Variables**
```bash
# my-service/.env (generated by Ansible)
SERVICE_NAME=my-service
SERVICE_HOST=my-service.lehel.xyz

# Reference in docker-compose.yml
labels:
  - "traefik.http.routers.${SERVICE_NAME}.rule=Host(`${SERVICE_HOST}`)"
```

### Naming Best Practices

1. **Directory Names:**
   - Use lowercase
   - Use hyphens for multi-word names (e.g., `achim-hoefer`)
   - Keep names short and descriptive
   - Avoid special characters

2. **Service Consistency:**
   - Directory name = Service name = URL subdomain
   - Maintains predictable structure
   - Easier troubleshooting

3. **DNS Compatibility:**
   - Valid DNS subdomain names only
   - No underscores (use hyphens)
   - Max 63 characters per label

4. **Traefik Labels:**
   - Always use `${SERVICE_NAME}` and `${SERVICE_HOST}` variables
   - Enables automatic service discovery
   - Simplifies multi-environment deployments

### Troubleshooting Service Names

**Service not accessible:**
```bash
# 1. Check .env file exists
ls -la /home/user/containers/{service}/.env
cat /home/user/containers/{service}/.env

# 2. Verify traefik routing
docker logs traefik | grep {service}

# 3. Check DNS resolution
dig {service}.lehel.xyz
nslookup {service}.lehel.xyz

# 4. Test traefik rule
curl -H "Host: {service}.lehel.xyz" http://localhost
```

**Wrong URL generated:**
```bash
# 1. Check inventory hostname
ansible-inventory --list | grep lehel.xyz

# 2. Regenerate .env files
cd ansible
ansible-playbook servyy.yml --tags "user.docker.env"

# 3. Restart service
cd /home/user/containers/{service}
docker-compose restart
```

---

## Services

### Core Infrastructure

| Service | Port | Purpose | Notes |
|---------|------|---------|-------|
| **traefik** | 80/443 | Reverse proxy, SSL termination | Routes all traffic, Let's Encrypt |
| **portainer** | 9000 | Docker UI | Container management interface |
| **monitor** | 9090 | Metrics collection | Prometheus + Grafana + exporters |
| **logging** | - | Log aggregation | Centralized logging (ELK-like) |
| **pass** | - | Password manager | Secrets management |
| **dns** | 53 | DNS filtering | PiHole + PostgreSQL |

### Applications

| Service | Port | Purpose | Database | Notes |
|---------|------|---------|----------|-------|
| **photoprism** | 8080 | Photo library | MariaDB | AI indexing, 10KB compose file |
| **social** | 4000 | Federated social | PostgreSQL | Pleroma (ActivityPub) |
| **git** | 8080 | Git hosting | - | Gitea-like with nginx |
| **jobs** | - | Job scheduling | - | Google API integration |
| **bumbleflies** | - | Custom app | - | With nginx |
| **achim-hoefer** | - | Static site | - | Parcel bundler |
| **energy** | - | Energy monitor | - | Home energy tracking |
| **me** | - | Personal site | - | Git submodule (dist branch) |

### Service Details

#### PhotoPrism
- **Purpose:** AI-powered photo library management
- **Features:** Face recognition, object detection, geo-tagging
- **Indexing:** Automated monthly indexing at 2:30 UTC
- **Batch Indexing:** Historical photos (2004-2022) via `index-photos-batch.sh`
- **Storage:** Photos on storagebox, database backed up daily
- **Size:** Largest docker-compose.yml (~10KB)

#### Social (Pleroma)
- **Purpose:** Self-hosted federated social network
- **Protocol:** ActivityPub (compatible with Mastodon)
- **Database:** PostgreSQL
- **Documentation:** See `social/README.md`
- **Configuration:** `social/environments/pleroma/pleroma.env`

#### DNS (PiHole)
- **Purpose:** Network-wide ad blocking and DNS
- **Features:** Custom DNS entries, DHCP server, ad filtering
- **Configuration:** `dns/pihole.env`
- **Custom Rules:** `dns/volumes/etc-dnsmasq.d/02-google_challenges.conf`

#### Monitor (Prometheus Stack)
- **Components:**
  - Prometheus: Metrics collection
  - Grafana: Visualization
  - Node Exporter: System metrics
  - cAdvisor: Container metrics
- **Configuration:** `monitor/prometheus.yml`
- **Scrape Targets:** localhost, cadvisor, node-exporter

#### Traefik
- **Purpose:** Ingress controller and reverse proxy
- **Features:**
  - Automatic service discovery via Docker labels
  - SSL/TLS with Let's Encrypt
  - HTTP to HTTPS redirect
  - Domain-based routing
- **Network:** External "proxy" network

---

## Ansible Automation

### Playbook Structure

```
ansible/servyy.yml (main entry point)
├── plays/system.yml    # System-level setup
├── plays/user.yml      # User + Docker services
├── plays/testing.yml   # Test environment
└── plays/leaguespehere.yml  # LeagueSphere prod
```

### Execution

```bash
cd ansible
./servyy.sh  # Runs: ansible-playbook servyy.yml -i production
```

### System Setup (plays/system.yml)

**Tasks:**
1. **Package Installation**
   - System: jq, curl, vim, git, zsh, locales, btop
   - Monitoring: monit
   - Storage: cifs-utils
   - Security: git-crypt
   - Kernel: linux-image-generic

2. **User Management**
   - Creates unprivileged user with sudo
   - SSH key-based authentication
   - Shell: zsh with zprezto

3. **Storage Configuration**
   - Mounts Hetzner Storagebox via CIFS
   - Mount point: `/mnt/storagebox`
   - Credentials encrypted in `vars/secrets.yml`

4. **Swap Setup**
   - Creates swap file (default 2GB)
   - Configurable per host
   - Only on hosts with `create_swap: true`

5. **Monitoring (monit)**
   - SSH daemon monitoring
   - System resources (CPU, memory, disk)
   - Storagebox mount verification
   - Docker container health checks
   - Email alerts on failures

### User/Docker Setup (plays/user.yml)

**Phase 1: Repository Setup**
```yaml
Tasks:
  - Clone shell config (zprezto)
  - Clone docker containers repository
  - Configure git credentials
  - Export/import git-crypt key
  - Unlock encrypted repositories
```

**Phase 2: Docker Installation**
```yaml
Tasks:
  - Install Docker CE + plugins
  - Configure daemon.json:
      - Local logging driver
      - Data root on 10GB volume (if available)
  - Create external "proxy" network for Traefik
  - Enable unprivileged user docker access
```

**Phase 3: Data Restoration**
```yaml
Tasks:
  - Restore git repos from storagebox
  - Restore photoprism database
  - Restore service configurations
```

**Phase 4: Service Deployment**
```yaml
Tasks:
  - Generate .env files per service
  - Start docker-compose for each service
  - Skip services tagged as "manual"
  - Enable systemd user linger
```

**Phase 5: Scheduled Tasks**
```yaml
Systemd Timers:
  - Photo indexing (2:30 UTC daily)
  - Home backup (daily)
  - Root backup (daily)
  - Photo backup (daily, conditional)
```

**Phase 6: DNS Setup**
```yaml
Tasks:
  - Initialize PiHole configuration
  - Set admin password
  - Configure upstream DNS
  - Add custom DNS entries
```

**Phase 7: Backup Configuration**
```yaml
Backup Targets:
  - Home: /mnt/storagebox/backup/{hostname}/home
  - Root: /mnt/storagebox/backup/{hostname}/root
  - Photos: (conditional per host)
Excludes:
  - .git, .cache, pgdata
  - Docker volumes
  - Temporary files
```

### Inventory (ansible/inventory/production)

**Production Hosts:**
```yaml
lehel.xyz:
  with_docker: true
  with_containers: true
  has_10g_volume: true      # Docker data root on extension
  create_swap: true

aqui.fritz.box:
  with_docker: true
  backup_photos: true
  ansible_user: cda
```

### Variables

**Default Variables (`plays/vars/default.yml`):**
- System packages list
- Storagebox mount path
- Swap configuration
- Docker configuration
- Backup schedules

**Secrets (`plays/vars/secrets.yml`):** (encrypted)
- Storagebox credentials
- Service passwords
- API keys

### Tags

```bash
# Run specific phases
ansible-playbook servyy.yml --tags "docker"
ansible-playbook servyy.yml --tags "backup"
ansible-playbook servyy.yml --tags "system"
```

---

## Scripts & Utilities

### Operational Scripts

#### `startup_docker_services.sh`
```bash
# Starts all docker-compose services
# Usage: ./startup_docker_services.sh
# Effect: docker-compose up -d in each service directory
```

#### `status_docker_services.sh`
```bash
# Shows status of all services
# Usage: ./status_docker_services.sh
# Output: Container status, health, uptime
```

#### `cleanup_space.sh`
```bash
# Cleans system to free space
# Actions:
#   - Vacuum journalctl (1 day retention)
#   - Remove unused Docker images
#   - Clean old kernel versions
```

#### `disable_systemd_services.sh`
```bash
# Disables unnecessary systemd services
# Usage: ./disable_systemd_services.sh
```

### Photo Management

#### `index-photos.sh`
```bash
# Index current month's photos
# Runs: docker exec photoprism photoprism index
# Scheduled: 2:30 UTC daily via systemd timer
```

#### `index-photos-batch.sh`
```bash
# Batch re-index historical photos
# Years: 2004-2022
# Uses: batch command for rate limiting
# Purpose: One-time historical indexing
```

### Test Infrastructure

#### `setup_test_container.sh`
```bash
# Creates LXD test container (servyy-test.lxd)
#
# Features:
#   - Creates LXD profile from servyy-test.yaml
#   - Creates storage pool
#   - Creates bridge network (lxdbr0)
#   - Launches Ubuntu container
#   - Enables security.privileged, security.nesting
#   - Configures DNS resolution for .lxd domain
#
# Options:
#   -x: Delete existing container first
#
# Usage:
#   ./setup_test_container.sh      # Create/start
#   ./setup_test_container.sh -x   # Recreate from scratch
```

#### `delete_test_container.sh`
```bash
# Removes test container
# Actions:
#   - Stop container
#   - Delete container
#   - Delete profile (optional)
```

### Hetzner Cloud Management

#### `hetzner/update_firewall.sh`
```bash
# Updates Hetzner Cloud firewall with current IP
#
# Process:
#   1. Detect current IPv4/IPv6
#   2. Extract router IPv6 via Avahi (if local)
#   3. Backup existing firewall rules
#   4. Update DNS access rules
#
# Use Case: Allow DNS queries from dynamic IP
```

#### `hetzner/main.py`
```bash
# Python CLI for Hetzner Cloud firewall
#
# Commands:
#   list                # List all firewalls
#   show <id>          # Show firewall details
#   update <id>        # Update firewall rules
#   backup <id>        # Backup firewall rules
#
# Configuration: Uses HETZNER_API_TOKEN env var
```

### Dynamic DNS

#### `duckdns/update_duckdns.sh` (DEPRECATED)
```bash
# LEGACY: Previously updated DuckDNS with current IP
# No longer used since domain purchased through Porkbun
# DNS now managed via Porkbun DNS console
#
# Historical reference only - can be removed
```

---

## Configuration Management

### Secrets Management (git-crypt)

**Setup:**
```bash
# On local machine (key holder)
git-crypt export-key /path/to/keyfile

# On remote server (during ansible deployment)
git-crypt unlock /path/to/keyfile
```

**Encrypted Files:**
- All `docker-compose.yml` files
- `.env` files with credentials
- `ansible/plays/vars/secrets.yml`
- Service-specific secrets (secret_google.json, etc.)

**Benefits:**
- Files encrypted at rest in git
- Transparent access after unlock
- No plaintext secrets in repository
- Automated unlock during deployment

### Environment Variables

**Template:** `ansible/plays/roles/user/templates/docker.env.j2`

```bash
SERVICE_NAME={{ service_name }}
HOSTNAME={{ service_name }}.lehel.xyz
PUID={{ ansible_user_uid }}
PGID={{ ansible_user_gid }}
TZ=Europe/Berlin
```

**Generated Per Service:**
- `/home/user/containers/{service}/.env`
- Used by docker-compose.yml
- Values from Ansible facts + inventory

### Service-Specific Configs

#### Nginx Configuration
```nginx
# Pattern: service/nginx.conf
# Services: git, me, bumbleflies

server {
    listen 80;
    server_name service.lehel.xyz;

    location / {
        proxy_pass http://localhost:port;
        proxy_set_header Host $host;
    }
}
```

#### Prometheus Configuration
```yaml
# monitor/prometheus.yml
scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  - job_name: 'cadvisor'
    static_configs:
      - targets: ['cadvisor:8080']

  - job_name: 'node'
    static_configs:
      - targets: ['node-exporter:9100']
```

#### Monit Configuration
```monit
# Generated by Ansible templates
# Location: /etc/monit/conf.d/

check process ssh with pidfile /var/run/sshd.pid
  start program = "/bin/systemctl start ssh"
  stop program = "/bin/systemctl stop ssh"
  if failed port 22 protocol ssh then restart

check filesystem storagebox with path /mnt/storagebox
  if space usage > 90% then alert
  if inode usage > 90% then alert

check program docker_containers with path /usr/local/bin/check_docker.sh
  if status != 0 then alert
```

---

## Networking

### Network Architecture

```
External Network (Internet)
    ↓
[Porkbun DNS: *.lehel.xyz → Server IP]
    ↓
[Hetzner Cloud Firewall: Allow 443, 80, 22, 53]
    ↓
[Server: lehel.xyz]
    ↓
[Traefik: 443/80]
    ↓
[Docker Network: proxy (external)]
    ↓
├─ photoprism.lehel.xyz → photoprism:8080
├─ git.lehel.xyz → git:8080
├─ social.lehel.xyz → social:4000
├─ grafana.lehel.xyz → monitor:3000
└─ [other services]

Internal Docker Networks
├─ proxy (external, shared)
├─ photoprism_default (internal)
├─ social_default (internal)
└─ [per-service networks]
```

### Traefik Routing

**Configuration Pattern (docker-compose labels):**
```yaml
labels:
  - "traefik.enable=true"
  - "traefik.http.routers.service.rule=Host(`service.lehel.xyz`)"
  - "traefik.http.routers.service.entrypoints=websecure"
  - "traefik.http.routers.service.tls.certresolver=letsencrypt"
  - "traefik.docker.network=proxy"
```

**Entry Points:**
- `web` (80): HTTP, redirects to HTTPS
- `websecure` (443): HTTPS with Let's Encrypt

### DNS Resolution

**Production:**
- External: DuckDNS → servyy.duckdns.org
- Internal: PiHole → local domains
- Traefik: Routes by hostname

**Test Environment:**
- LXD DNS: `.lxd` domain resolution
- `/etc/hosts` entries for test services
- `resolvectl` configuration for lxdbr0

**PiHole Setup:**
```bash
# Custom DNS entries
photoprism.lehel.xyz → 192.168.1.x
git.lehel.xyz → 192.168.1.x
[other services]

# Upstream DNS
Cloudflare: 1.1.1.1, 1.0.0.1
Google: 8.8.8.8, 8.8.4.4
```

### Port Mapping

| Service | Internal Port | External Port | Protocol | Notes |
|---------|--------------|---------------|----------|-------|
| Traefik | 80/443 | 80/443 | HTTP/HTTPS | Entry point |
| PiHole | 53 | 53 | DNS/UDP | Network DNS |
| PhotoPrism | 8080 | - | HTTP | Via Traefik |
| Social | 4000 | - | HTTP | Via Traefik |
| Git | 8080 | - | HTTP | Via Traefik |
| Portainer | 9000 | 9000 | HTTP | Direct access |
| Prometheus | 9090 | - | HTTP | Via Traefik |
| Grafana | 3000 | - | HTTP | Via Traefik |

### Firewall Rules

**Hetzner Cloud Firewall:**
```yaml
Inbound Rules:
  - 22/tcp (SSH) - from whitelisted IPs
  - 80/tcp (HTTP) - from anywhere
  - 443/tcp (HTTPS) - from anywhere
  - 53/udp (DNS) - from dynamic IPs

Outbound Rules:
  - All traffic allowed
```

**Local Firewall:**
- Managed by Hetzner Cloud
- Updated via `hetzner/update_firewall.sh`
- Backup before changes

---

## Deployment

### Production Deployment

**Full Deployment:**
```bash
cd ansible
./servyy.sh
```

**Selective Deployment:**
```bash
# System setup only
ansible-playbook servyy.yml --tags "system"

# Docker services only
ansible-playbook servyy.yml --tags "docker"

# Backup configuration only
ansible-playbook servyy.yml --tags "backup"

# Specific host
ansible-playbook servyy.yml --limit lehel.xyz
```

### Test Container Deployment

**Create Test Environment:**
```bash
cd scripts
./setup_test_container.sh
```

**Deploy to Test:**
```bash
cd ansible
ansible-playbook testing.yml -i testing_inventory
```

**Cleanup:**
```bash
cd scripts
./delete_test_container.sh
```

### Deployment Flow

```
1. Ansible System Playbook
   ├─ Install packages
   ├─ Create user
   ├─ Mount storagebox
   ├─ Configure swap
   └─ Setup monit

2. Ansible User Playbook
   ├─ Clone repositories
   ├─ Unlock git-crypt
   ├─ Install Docker
   ├─ Restore backups
   ├─ Generate .env files
   ├─ Start services
   └─ Configure timers

3. Service Initialization
   ├─ Traefik starts (proxy network)
   ├─ DNS starts (PiHole)
   ├─ Monitor starts (Prometheus)
   ├─ Applications start
   └─ Health checks pass

4. Post-Deployment
   ├─ Verify DNS resolution (Porkbun)
   ├─ Update Hetzner firewall (if needed)
   ├─ Verify all services
   └─ Check monitoring
```

### Environment Requirements

**Production Server:**
- Ubuntu 22.04 LTS (or similar)
- Docker-capable hardware
- Network access to Hetzner Storagebox
- SSH access with key authentication
- Optional: 10GB extension volume

**Test Container (LXD):**
- LXD installed on host
- Storage pool available
- Bridge network configured
- Sufficient memory/CPU

### Pre-Deployment Checklist

- [ ] Hetzner Storagebox accessible
- [ ] git-crypt key exported
- [ ] Ansible inventory updated
- [ ] Porkbun DNS configured (A/AAAA records, wildcard)
- [ ] SSH keys configured
- [ ] Sufficient disk space
- [ ] Docker-capable kernel

### Post-Deployment Verification

```bash
# Check services
./scripts/status_docker_services.sh

# Verify traefik routing
curl -I https://photoprism.lehel.xyz

# Check monitoring
curl http://localhost:9090  # Prometheus
curl http://localhost:3000  # Grafana

# Verify DNS
dig @localhost example.com

# Check backups
ls -lh /mnt/storagebox/backup/$(hostname)/
```

---

## Monitoring & Backup

### Monitoring Stack

#### Monit (System Level)

**Configuration:** `/etc/monit/conf.d/`

**Checks:**
```monit
# SSH Service
check process ssh
  if failed port 22 protocol ssh then restart

# System Resources
check system $HOST
  if loadavg (1min) > 4 then alert
  if loadavg (5min) > 2 then alert
  if cpu usage > 95% for 10 cycles then alert
  if memory usage > 90% then alert

# Filesystem
check filesystem rootfs with path /
  if space usage > 90% then alert

check filesystem storagebox with path /mnt/storagebox
  if not mounted then alert

# Docker Containers (per service)
check program docker_containers
  if status != 0 then alert
```

**Alerts:** Email notifications on failures

#### Prometheus Stack

**Components:**
- **Prometheus:** Time-series metrics database
- **Grafana:** Visualization and dashboards
- **Node Exporter:** System metrics (CPU, memory, disk, network)
- **cAdvisor:** Container metrics (per-container resources)

**Metrics Collected:**
- System: CPU, memory, disk I/O, network
- Containers: Resource usage, restart count, health
- Services: Application-specific metrics

**Scrape Interval:** 15 seconds (default)

**Retention:** 15 days (default)

**Dashboards:**
- System Overview
- Docker Containers
- Service-specific dashboards

#### Logging

**Service:** Centralized logging (logging container)

**Collection:**
- Docker logs via local logging driver
- System logs via journald
- Application logs via stdout/stderr

**Storage:** Local volumes (for performance)

**Retention:** Configurable per service

### Backup Strategy

#### Backup Schedule (Systemd Timers)

| Backup Type | Frequency | Time (UTC) | Target |
|-------------|-----------|------------|--------|
| Home directory | Daily | 03:00 | /mnt/storagebox/backup/{hostname}/home |
| Root filesystem | Daily | 04:00 | /mnt/storagebox/backup/{hostname}/root |
| Photos | Daily | 05:00 | /mnt/storagebox/backup/{hostname}/photos |
| PhotoPrism DB | Daily | 02:00 | /mnt/storagebox/backup/{hostname}/photoprism |

#### Backup Method: rsync

**Command Pattern:**
```bash
rsync -av --delete \
  --exclude='.git' \
  --exclude='.cache' \
  --exclude='pgdata' \
  /source/ \
  /mnt/storagebox/backup/$(hostname)/target/
```

**Features:**
- Incremental backups
- Bandwidth efficient
- Preserves permissions
- Automatic cleanup (--delete)

#### Backup Exclusions

**Home Directory:**
- `.git/` - Git repositories
- `.cache/` - Cache files
- `.local/share/docker/` - Docker data
- `pgdata/` - PostgreSQL data (backed up separately)
- `.npm/` - npm cache
- `.pyenv/` - Python environments

**Root Filesystem:**
- `/tmp/` - Temporary files
- `/var/cache/` - System cache
- `/var/tmp/` - Temporary files
- `/proc/`, `/sys/`, `/dev/` - Virtual filesystems
- `/mnt/storagebox/` - Backup destination itself

#### Database Backups

**PhotoPrism Database:**
```bash
# Dump MariaDB database
docker exec photoprism-db mysqldump \
  -u photoprism -p \
  photoprism > /backup/photoprism.sql

# Sync to storagebox
rsync -av /backup/photoprism.sql \
  /mnt/storagebox/backup/photoprism/
```

**Social (Pleroma) Database:**
```bash
# PostgreSQL dump
docker exec social-db pg_dump \
  -U pleroma pleroma > /backup/pleroma.sql
```

#### Restore Procedures

**Home Directory Restore:**
```bash
rsync -av \
  /mnt/storagebox/backup/{hostname}/home/ \
  /home/user/
```

**Database Restore:**
```bash
# PhotoPrism
docker exec -i photoprism-db mysql \
  -u photoprism -p photoprism < backup.sql

# Social (Pleroma)
docker exec -i social-db psql \
  -U pleroma pleroma < backup.sql
```

**Full System Restore:**
1. Fresh OS installation
2. Run ansible playbook (system.yml)
3. Run ansible playbook (user.yml)
   - Automatically restores from storagebox
4. Verify services

#### Backup Verification

**Manual Verification:**
```bash
# Check backup size
du -sh /mnt/storagebox/backup/$(hostname)/*

# Check backup age
ls -lh /mnt/storagebox/backup/$(hostname)/*/

# Verify backup contents
rsync -avn --delete /source/ /backup/
```

**Automated Verification:**
- Monit monitors storagebox mount
- Ansible playbook verifies backup completion
- Email alerts on backup failures

### Disaster Recovery

**Scenario: Complete Server Loss**

1. **Provision New Server:**
   ```bash
   # Install Ubuntu 22.04 LTS
   # Configure SSH access
   ```

2. **Run Ansible Playbooks:**
   ```bash
   cd ansible
   ./servyy.sh
   ```

3. **Verify Restoration:**
   ```bash
   ./scripts/status_docker_services.sh
   ```

**Recovery Time Objective (RTO):** ~2 hours
**Recovery Point Objective (RPO):** 24 hours (daily backups)

**Scenario: Service Failure**

1. **Check Container Status:**
   ```bash
   docker ps -a
   docker logs {container}
   ```

2. **Restart Service:**
   ```bash
   cd /home/user/containers/{service}
   docker-compose restart
   ```

3. **Restore from Backup (if needed):**
   ```bash
   rsync -av /mnt/storagebox/backup/{hostname}/{service}/ .
   docker-compose up -d
   ```

**Scenario: Data Corruption**

1. **Stop Service:**
   ```bash
   docker-compose stop
   ```

2. **Restore Data:**
   ```bash
   rsync -av /mnt/storagebox/backup/{hostname}/{service}/data/ ./data/
   ```

3. **Restart Service:**
   ```bash
   docker-compose up -d
   ```

---

## Development Workflow

### Local Development

**Setup:**
```bash
# Clone repository
git clone <repo-url> servyy-container
cd servyy-container

# Setup git-crypt (requires key from key holder)
git-crypt unlock /path/to/keyfile

# Install Ansible
pip install ansible

# Install Ansible requirements
cd ansible
ansible-galaxy install -r requirements.yml
```

**Test in LXD Container:**
```bash
# Create test container
cd scripts
./setup_test_container.sh

# Deploy to test
cd ../ansible
ansible-playbook testing.yml -i testing_inventory

# SSH to test container
ssh servyy-test.lxd

# Cleanup
cd ../scripts
./delete_test_container.sh
```

### Git Workflow

**Branches:**
- `main` - Production-ready code
- `claude/*` - Claude Code development branches
- Feature branches as needed

**Commit Guidelines:**
```
Format: <type>: <description>

Types:
  - feat: New feature
  - fix: Bug fix
  - chore: Maintenance
  - refactor: Code restructure
  - docs: Documentation

Example:
  feat: add logging container for centralized logs
  chore: update traefik configuration
  fix: photoprism indexing script timeout
```

### Adding a New Service

**Step 1: Create Service Directory**
```bash
mkdir {service-name}
cd {service-name}
```

**Step 2: Create docker-compose.yml**
```yaml
version: '3.8'

services:
  {service-name}:
    image: {docker-image}
    container_name: {service-name}
    restart: unless-stopped
    networks:
      - proxy
    environment:
      - PUID=${PUID}
      - PGID=${PGID}
    volumes:
      - ./data:/data
      - ./config:/config
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.{service}.rule=Host(`{service}.lehel.xyz`)"
      - "traefik.http.routers.{service}.entrypoints=websecure"
      - "traefik.http.routers.{service}.tls.certresolver=letsencrypt"

networks:
  proxy:
    external: true
```

**Step 3: Encrypt with git-crypt**
```bash
# Add to .gitattributes (if not already present)
echo "{service-name}/** filter=git-crypt diff=git-crypt" >> .gitattributes

# Commit
git add {service-name}/
git commit -m "feat: add {service-name} service"
```

**Step 4: Update Ansible Playbook**
```yaml
# ansible/plays/roles/user/tasks/docker_services.yml
- name: Start {service-name}
  docker_compose:
    project_src: "{{ containers_path }}/{service-name}"
    state: present
  tags: [docker, {service-name}]
```

**Step 5: Deploy**
```bash
cd ansible
ansible-playbook servyy.yml --tags "{service-name}"
```

### Testing Changes

**Unit Testing (Ansible):**
```bash
# Syntax check
ansible-playbook servyy.yml --syntax-check

# Dry run
ansible-playbook servyy.yml --check

# Run on test container
ansible-playbook testing.yml -i testing_inventory
```

**Integration Testing:**
```bash
# Deploy to test container
./scripts/setup_test_container.sh
cd ansible
ansible-playbook testing.yml

# Verify services
ssh servyy-test.lxd
docker ps
curl http://localhost

# Cleanup
cd ../scripts
./delete_test_container.sh
```

### Troubleshooting

**Service Won't Start:**
```bash
# Check logs
docker logs {container-name}

# Check compose file
cd /home/user/containers/{service}
docker-compose config

# Check .env file
cat .env

# Try manual start
docker-compose up
```

**Network Issues:**
```bash
# Check networks
docker network ls
docker network inspect proxy

# Check traefik routing
docker logs traefik
curl -I https://{service}.lehel.xyz
```

**Backup Failures:**
```bash
# Check storagebox mount
mount | grep storagebox
ls -lh /mnt/storagebox

# Check systemd timers
systemctl --user list-timers

# Check service logs
journalctl --user -u backup-home.service
```

**Git-crypt Issues:**
```bash
# Check encryption status
git-crypt status

# Re-unlock
git-crypt unlock /path/to/keyfile

# Verify encrypted files
git-crypt status | grep -v "not encrypted"
```

### Maintenance Tasks

**Regular Maintenance:**
```bash
# Weekly: Clean Docker
docker system prune -af

# Weekly: Update packages
apt update && apt upgrade

# Monthly: Check backups
ls -lh /mnt/storagebox/backup/$(hostname)/

# Monthly: Review monitoring alerts
# Check monit dashboard
# Review Prometheus alerts
# Check Grafana dashboards
```

**Security Updates:**
```bash
# Update Docker images
cd /home/user/containers/{service}
docker-compose pull
docker-compose up -d

# Update system packages
apt update && apt upgrade

# Review firewall rules
cd hetzner
python main.py list
```

**Performance Optimization:**
```bash
# Check disk usage
df -h
du -sh /home/user/containers/*

# Check resource usage
docker stats

# Review logs size
journalctl --disk-usage
```

---

## Quick Reference

### Common Commands

```bash
# Service Management
./scripts/startup_docker_services.sh      # Start all services
./scripts/status_docker_services.sh       # Check status
docker-compose restart {service}          # Restart service

# Monitoring
docker ps                                 # Container status
docker logs -f {container}               # Follow logs
systemctl --user list-timers             # Check timers

# Backup
rsync -avn /source/ /backup/             # Dry run backup
ls -lh /mnt/storagebox/backup/           # Check backups

# Ansible
cd ansible && ./servyy.sh                # Full deployment
ansible-playbook servyy.yml --tags "X"   # Partial deployment

# Testing
./scripts/setup_test_container.sh        # Create test env
./scripts/delete_test_container.sh       # Cleanup test env
```

### File Locations

```bash
# Containers
/home/user/containers/{service}/          # Service directories

# Configuration
/home/user/containers/{service}/.env      # Environment vars
/etc/monit/conf.d/                       # Monit configs

# Logs
docker logs {container}                   # Container logs
journalctl -u {service}                  # Systemd logs

# Backups
/mnt/storagebox/backup/{hostname}/       # All backups

# Scripts
/home/user/servyy-container/scripts/     # Utility scripts
```

### Important URLs

```bash
# Production (via Porkbun DNS)
https://photoprism.lehel.xyz             # PhotoPrism
https://git.lehel.xyz                    # Git hosting
https://social.lehel.xyz                 # Pleroma
https://grafana.lehel.xyz                # Grafana
https://lehel.xyz                        # Main domain

# Local Access
http://localhost:9000                    # Portainer
http://localhost:9090                    # Prometheus
http://localhost:3000                    # Grafana
```

### Contact & Support

For issues, improvements, or questions:
- Review this documentation
- Check service logs: `docker logs {container}`
- Review monitoring: Grafana dashboards
- Check backups: `/mnt/storagebox/backup/`

---

**Document Version:** 1.0
**Last Updated:** 2025-11-03
**Maintained by:** Infrastructure automation (Ansible)
